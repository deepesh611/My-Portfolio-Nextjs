[
  {
    "slug": "aetherstore",
    "metadata": {
      "title": "AetherStore",
      "publishedAt": "2026-01-02",
      "summary": "A lightweight, scalable, S3-style distributed object storage system written in Go. Unlike Hadoop/HDFS, AetherStore treats files as immutable objects and uses only Go's standard library plus SQLite for metadata. Currently supports standalone DataNode and Master Node services with Docker Compose orchestration.",
      "image": "",
      "images": [
        "/images/projects/aetherstore/cover-02.png",
        "/images/projects/aetherstore/cover-01.png"
      ],
      "tag": [],
      "team": [
        {
          "name": "Deepesh Patil",
          "role": "Software Engineer",
          "avatar": "/images/avatar.jpg",
          "linkedin": "https://www.linkedin.com/indeepesh-patil-103a87258"
        }
      ],
      "link": "https://github.com/deepesh611/AetherStore"
    },
    "content": "\n## Overview\n\nAetherStore is a distributed object storage platform designed for immutable, durable object storage across multiple nodes. The system separates concerns between storage workers (DataNodes) and metadata management (Master Node), using SQLite for metadata persistence and local filesystem storage for objects. It provides REST APIs for object lifecycle management, supports replication through random node placement, and is fully Dockerized for deployment.\n\n## Key Features\n\n- Durable writes with fsync guarantees on DataNode\n- Immutable object storage with UUIDv4 identifiers\n- REST API for PUT/GET/health operations on DataNode\n- SQLite database with WAL mode for metadata\n- Node registration and tracking via Master Node\n- Random placement algorithm for replica distribution\n- Docker Compose orchestration for multi-node deployment\n- Persistent volume support for data durability\n\n## Design Decisions\n\n- Separated storage and metadata concerns between DataNode and Master Node for scalability\n- Used SQLite with WAL mode instead of external databases to minimize dependencies and complexity\n- Implemented immutable object storage to simplify consistency and enable safe concurrent access\n- Chose random placement over consistent hashing for simpler implementation and adequate distribution\n- Built with Go standard library only (plus SQLite driver) to reduce operational overhead\n- Designed crash-safe durability through fsync and atomic file operations\n- Used Docker Compose for orchestration to enable local development and testing of multi-node scenarios\n\n![Data Storage Architecture](/images/projects/aetherstore/cover-03.png)\n\n## Technologies Used\n\n- Go (stdlib + SQLite driver)\n- HTTP/JSON protocols\n- SQLite with WAL mode for metadata\n- Local filesystem storage with fsync\n- Docker and Docker Compose\n- RESTful API design"
  },
  {
    "slug": "dbms-with-n8n",
    "metadata": {
      "title": "Member Management System (Workflow-Driven Backend)",
      "publishedAt": "2024-09-15",
      "summary": "A workflow-driven member management system built with React and n8n, delivered as part of a freelance engagement. Implements CRUD operations, relational data modeling, and analytics using a self-hosted stack.",
      "image": "",
      "images": [
        "/images/about/freelance/01.png",
        "/images/about/freelance/02.png",
        "/images/about/freelance/03.png",
        "/images/about/freelance/04.png",
        "/images/about/freelance/05.png",
        "/images/about/freelance/06.png"
      ],
      "tag": [],
      "team": [
        {
          "name": "Deepesh Patil",
          "role": "Full Stack Developer",
          "avatar": "/images/avatar.jpg",
          "linkedin": "https://www.linkedin.com/in/deepesh-patil-103a87258"
        }
      ],
      "link": "https://github.com/deepesh611/DBMS-with-n8n"
    },
    "content": "\n> **Note:** This project was developed as part of a freelance engagement. Client details have been anonymized, and only non-sensitive functionality is shown.\n\n## Overview\n\nThis project is a **workflow-driven Member Management System** designed to replace manual and spreadsheet-based record keeping with a structured, queryable platform.\n\nThe system uses a **React + TypeScript frontend** paired with **n8n workflows acting as the backend layer**, handling CRUD operations, validation, and automation. All data is stored in a **MySQL database**, and the application is designed to be deployed in a self-hosted environment.\n\nA public demo is provided to showcase functionality and UI behavior.\n\n[**View Demo (Showcase Instance)**](https://dbms-with-n8n.vercel.app/)\n\n---\n\n## Problem This Project Solves\n\nOrganizations that rely on spreadsheets or fragmented tools often struggle with:\n- Maintaining consistent member records\n- Representing family and relational data\n- Generating demographic insights\n- Ensuring control over their data infrastructure\n\nThis project addresses those issues by providing a centralized system with structured data models, automation-driven backend logic, and built-in analytics.\n\n---\n\n## Core Capabilities\n\n- **CRUD-based Member Management:** Structured creation, retrieval, updating, and deletion of member records\n- **Relational Data Modeling:** Family relationships (spouse, parent, child) represented through normalized tables\n- **Employment Tracking:** Storage of professional and employment-related attributes\n- **Analytics & Insights:** Demographic and employment summaries rendered through dashboards\n- **Image Handling:** Support for profile images via uploads or external links\n- **Bulk Operations:** CSV/XLSX-based data import for migration from existing records\n\n---\n\n## Tech Stack\n\nThis project follows a **low-code backend / pro-code frontend** approach:\n\n- **Frontend:** React + Vite + TypeScript\n- **UI & Styling:** Tailwind CSS + shadcn/ui\n- **Backend Logic:** n8n workflows exposed via webhooks\n- **Database:** MySQL\n- **Visualization:** Recharts\n\n---\n\n## Architecture\n\nThe system is designed around a **workflow-centric backend**:\n\n1. **React Frontend**  \n   Sends requests directly to n8n webhook endpoints.\n\n2. **n8n Workflows**  \n   Act as the API layer, handling validation, business logic, and SQL execution.\n\n3. **MySQL Database**  \n   Stores normalized relational data and supports complex queries.\n\nThis approach avoids maintaining a large custom backend codebase while keeping logic explicit and modular.\n\n---\n\n## Database Design\n\nThe schema is normalized to support real-world relationships:\n\n- **Members:** Core identity and status information\n- **Phones:** One-to-many contact records (primary, WhatsApp, emergency)\n- **Employment:** Professional and employment details\n- **Relationships:** Self-referential links defining family connections\n\n---\n\n## Trade-offs & Limitations\n\n- Designed for internal or small-scale organizational use\n- No authentication or role-based access control\n- Workflow-based backend prioritizes flexibility over raw performance\n- Demo deployment is for showcase purposes only\n\n---\n\n## Outcome & Learnings\n\n- Delivered a complete CRUD-based system for a real client\n- Gained experience using workflow automation as backend infrastructure\n- Designed and implemented relational schemas for non-trivial data\n- Balanced rapid development with maintainable system design\n"
  },
  {
    "slug": "DDoS-Project",
    "metadata": {
      "title": "Cloud DDoS Detection using Machine Learning (Research Project)",
      "publishedAt": "2024-10-17",
      "summary": "A research-focused study exploring machine learningâ€“based anomaly detection techniques for identifying DDoS attacks in cloud environments using dataset-driven and simulated traffic analysis.",
      "image": "",
      "images": [],
      "tag": [],
      "team": [
        {
          "name": "Deepesh Patil",
          "role": "Software Engineer",
          "avatar": "/images/avatar.jpg",
          "linkedin": "https://www.linkedin.com/in/deepesh-patil-103a87258"
        },
        {
          "name": "Saksham Dharmik",
          "role": "Software Engineer",
          "avatar": "https://avatars.githubusercontent.com/u/123961289?v=4"
        },
        {
          "name": "Tanishq Ingole",
          "role": "Software Engineer",
          "avatar": "https://avatars.githubusercontent.com/u/157146424?v=4"
        }
      ],
      "link": "https://github.com/deepesh611/Minor-Project-DDoS-on-Cloud"
    },
    "content": "\n## Overview\n\nThis project is a **research-focused study on detecting Distributed Denial of Service (DDoS) attacks in cloud environments** using machine learningâ€“based anomaly detection techniques.\n\nThe work explores how abnormal traffic patterns can be identified by analyzing network behavior and how such detection mechanisms can support cloud service providers in improving availability and resilience.  \nThe emphasis of this project is **detection, analysis, and research**, rather than production-grade deployment or live mitigation.\n\n---\n\n## Problem Statement\n\nCloud platforms are frequent targets of DDoS attacks due to their public accessibility and shared infrastructure. These attacks aim to overwhelm cloud services with malicious traffic, leading to service degradation or outages.\n\nTraditional rule-based security mechanisms struggle to adapt to evolving attack patterns, making **data-driven detection approaches** an important area of study for modern cloud security.\n\n---\n\n## Project Objectives\n\n- Study the behavior and characteristics of DDoS attacks in cloud environments  \n- Explore machine learning techniques for anomaly detection in network traffic  \n- Design a conceptual detection pipeline suitable for cloud-based systems  \n- Evaluate detection effectiveness using datasets and simulated traffic scenarios  \n\n---\n\n## Scope and Limitations\n\n**In Scope**\n- Research-oriented DDoS detection  \n- Machine learningâ€“based traffic analysis  \n- Conceptual prevention and response strategies  \n\n**Out of Scope**\n- Live traffic mitigation  \n- Production deployment on real cloud infrastructure  \n- Real-time firewall or load balancer integration  \n\n---\n\n## System Architecture (Conceptual)\n\nThe proposed detection pipeline follows these stages:\n\n1. **Traffic Data Collection**  \n   Network traffic data is collected from datasets or simulated cloud traffic representing both normal and attack scenarios.\n\n2. **Feature Extraction**  \n   Relevant traffic attributes such as request rates, packet patterns, and statistical anomalies are extracted.\n\n3. **Machine Learningâ€“Based Detection**  \n   Models are trained to differentiate between normal traffic behavior and anomalous patterns indicative of DDoS attacks.\n\n4. **Attack Classification & Alerting**  \n   Detected anomalies are classified as potential DDoS events and flagged for further analysis or response.\n\n---\n\n## Machine Learning Approach\n\nThe project explores **machine learningâ€“based anomaly detection**, focusing on:\n\n- Pattern recognition in network traffic  \n- Differentiation between legitimate traffic spikes and malicious flooding  \n- Adaptive learning concepts for evolving attack patterns  \n\nDeep learning and incremental learning concepts are explored at a **theoretical and experimental level**, without large-scale neural network deployment.\n\n---\n\n## Tools & Technologies\n\n- **Programming Language:** Python  \n- **Domain:** Cybersecurity, Cloud Computing  \n- **Techniques:** Machine Learning, Anomaly Detection  \n- **Artifacts:** Research Paper, BTP Report, Documentation  \n\n---\n\n## Results & Observations\n\n- ML-based detection shows promise in identifying abnormal traffic behavior  \n- Data-driven approaches demonstrate better adaptability compared to static rules  \n- Detection accuracy is highly dependent on dataset quality and feature selection  \n\n> Note: Quantitative performance metrics are discussed in the accompanying research documentation rather than implemented as production benchmarks.\n\n## Research Artifacts\n\n- ðŸ“„ **Research Paper:**  \n  https://deepesh611.github.io/DDoS-on-Cloud-Research/DDoS_Project_Paper.pdf  \n  *(Content based on original project report; AI used only for language refinement and formatting.)*\n\n---\n\n## Future Enhancements\n\n- Integration with real cloud traffic sources  \n- Exploration of advanced deep learning architectures  \n- Investigation of real-time detection and mitigation strategies  \n- Honeypot-based traffic analysis as a defensive extension  \n\n## Key Takeaway\n\nThis project demonstrates a **research-driven understanding of DDoS detection in cloud environments**, emphasizing analytical thinking, system design, and applied machine learning concepts rather than production deployment.\n"
  },
  {
    "slug": "Home-Server",
    "metadata": {
      "title": "Home Server Guide",
      "publishedAt": "2025-12-12",
      "summary": "A comprehensive, beginner-friendly guide to building a self-hosted home server using modern tools like Docker, Portainer, Twingate, and Nginx Proxy Manager. Walks through hardware preparation, secure remote access, containerized deployments, and service dashboards.",
      "image": "",
      "images": [
        "/images/projects/home-server/cover-01.png",
        "/images/projects/home-server/cover-02.png",
        "/images/projects/home-server/cover-03.png"
      ],
      "tag": [],
      "team": [
        {
          "name": "Deepesh Patil",
          "role": "Software Engineer",
          "avatar": "/images/avatar.jpg",
          "linkedin": "https://www.linkedin.com/deepesh-patil-103a87258"
        }
      ],
      "link": "https://github.com/deepesh611/Home-Server"
    },
    "content": "\n## Overview\n\nThis project is a **comprehensive home server setup guide** that walks readers through turning a spare device â€” such as an old laptop, Raspberry Pi, or desktop â€” into a self-hosted server capable of running services like media servers, dashboards, and containerized applications.\n\nThe guide is written in a practical, beginner-friendly style and emphasizes clear, step-by-step documentation rather than abstract theory. The focus is on using modern, approachable tools to make self-hosting more accessible.\n\n---\n\n## Why This Guide\n\nSetting up a home server can feel intimidating for many people due to scattered resources and complex tutorials online. This guide exists to:\n\n- Break down the setup process into clear, incremental sections  \n- Teach secure remote access without exposing open ports  \n- Demonstrate how to use Docker and Portainer for service management  \n- Provide examples for building a unified landing page and SSL-secured routing  \n\nIt is aimed at people who want to learn hands-on home-server setup without unnecessary jargon.\n\n---\n\n## What the Guide Covers\n\nThe guide leads users through the following stages:\n\n- Preparing the hardware and flashing a Linux-based OS  \n- Assigning a static IP and enabling SSH for remote management  \n- Setting up secure remote access using Twingate  \n- Installing Docker for managing applications as containers  \n- Using Portainer's web UI to manage containers  \n- Building a custom landing page for self-hosted services  \n- Configuring Nginx Proxy Manager for SSL and domain routing  \n- Deploying services such as media servers, file browsers, or dashboards  \n\nEach section is designed to be accessible and practical, with links and instructions to help readers follow along at their own pace.\n\n---\n\n## Key Tools and Technologies\n\nThis guide demonstrates the use of:\n\n- **Docker** for containerizing applications  \n- **Portainer** for visual container management  \n- **Twingate** for secure remote access without opening router ports  \n- **Nginx Proxy Manager** for SSL certificates and domain routing  \n- **Linux-based OS** setup for initial server configuration  \n- Shell scripts and automation examples  \n- Example dashboards built with Astro.js / Next.js  \n\nThese tools were selected for their usability and broad adoption in self-hosted environments, making the guide suitable for beginners and intermediate users alike.\n\n---\n\n## Approach and Design Decisions\n\n- Focused on step-by-step documentation rather than software development  \n- Emphasized tools that are user-friendly and reduce reliance on terminal commands where possible  \n- Included secure remote access patterns to avoid common security pitfalls  \n- Organized content to build confidence progressively  \n\nThe guide is a learning resource as well as a reference manual for people setting up their first home server.\n\n---\n\n## Limitations\n\n- This is **not a packaged application or dashboard product**  \n- It does **not include a complete automated installer**  \n- The guide assumes some familiarity with concepts like networking and Linux basics  \n\nReaders looking for hands-off \"one-click\" home server platforms might want to explore dedicated OS projects (e.g., CasaOS, Umbrel).\n\n---\n\n## Outcome & Value\n\n- Offers a clear, structured approach to home server setup  \n- Helps users gain confidence with remote access, Docker, and service management  \n- Serves as a learning resource for self-hosting fundamentals  \n- Provides practical steps for secure and meaningful home infrastructure  \n"
  },
  {
    "slug": "multi-agent-fraud-intelligence-system",
    "metadata": {
      "title": "Multi-Agent Fraud Detection System",
      "publishedAt": "2025-11-30",
      "summary": "A hybrid fraud detection platform combining rule-based analytics and AI agents to detect, investigate, and explain fraudulent insurance claims through an interactive dashboard.",
      "image": "",
      "images": [
        "/images/projects/multi-agent-system/cover-01.png",
        "/images/projects/multi-agent-system/cover-02.png",
        "/images/projects/multi-agent-system/cover-03.png",
        "/images/projects/multi-agent-system/cover-04.png"
      ],
      "tag": [],
      "team": [
        {
          "name": "Deepesh Patil",
          "role": "Software Engineer",
          "avatar": "/images/avatar.jpg",
          "linkedin": "https://www.linkedin.com/in/deepesh-patil-103a87258"
        }
      ],
      "link": "https://github.com/deepesh611/Multi-Agent-Fraud-Detection-System"
    },
    "content": "\r\n## Overview\r\n\r\nThe **Multi-Agent Fraud Detection System** is a hybrid AI-driven platform designed to detect, investigate, and explain fraudulent insurance claims.  \r\nIt combines **rule-based detection**, **AI-powered agents**, and an **interactive dashboard** to provide transparent and explainable fraud analysis.\r\n\r\nThe system focuses on decomposing fraud detection into specialized agents rather than relying on a single monolithic model.\r\n\r\n---\r\n\r\n## What the Project Includes\r\n\r\n- **Synthetic Data Pipeline** for generating realistic insurance claims with injected fraud patterns\r\n- **Rule-Based Detection Engine** identifying suspicious claim behavior\r\n- **Multi-Agent Architecture**, including:\r\n  - Detection Agent (rule-based analysis)\r\n  - Investigation Agent (AI-driven reasoning)\r\n  - Explanation Agent (human-readable fraud justification)\r\n  - Query Agent (natural language search over historical cases)\r\n- **Interactive Dashboard** for fraud analytics, claim exploration, and AI-assisted investigation\r\n- **End-to-End Pipeline** from data generation to fraud verdict\r\n\r\n---\r\n\r\n## Key Features\r\n\r\n- Hybrid fraud detection using rules and AI agents\r\n- Explainable decisions with traceable reasoning\r\n- Modular agent design for independent evolution\r\n- Natural language querying of claims data\r\n- Interactive visual analytics dashboard\r\n\r\n---\r\n\r\n## Architecture\r\n\r\nThe system follows a coordinated multi-agent design:\r\n\r\n1. **Synthetic Data Generator** produces labeled claim datasets  \r\n2. **ETL Pipeline** processes and stores data  \r\n3. **Agent Orchestrator** coordinates agent execution  \r\n4. **Specialized Agents** analyze, investigate, and explain fraud  \r\n5. **Dashboard UI** presents analytics and agent outputs  \r\n\r\nThis architecture emphasizes **modularity**, **fault isolation**, and **decision transparency**.\r\n\r\n---\r\n\r\n## Tools & Technologies\r\n\r\n- **Python** â€” core implementation\r\n- **Streamlit** â€” interactive dashboard UI\r\n- **SQLite** â€” data storage\r\n- **FAISS** â€” vector similarity search\r\n- **LLMs** â€” reasoning and explanation agents\r\n- **Docker** â€” containerized execution\r\n\r\n---\r\n\r\n## Current Status\r\n\r\n- Functional prototype with multiple cooperating agents\r\n- Working dashboard for analytics and investigation\r\n- Synthetic test data and simulated fraud scenarios\r\n- Modular architecture ready for extension\r\n\r\n---\r\n\r\n## Outcome & Learnings\r\n\r\n- Designed a real multi-agent AI system end-to-end\r\n- Combined deterministic rules with AI reasoning\r\n- Built explainable fraud detection pipelines\r\n- Gained practical experience in AI orchestration and system design"
  },
  {
    "slug": "videodigest",
    "metadata": {
      "title": "Video Summary Generator",
      "publishedAt": "2025-10-26",
      "summary": "A multimodal AI system that generates concise summaries from videos by running parallel pipelines for visual frame analysis and audio-based transcription and understanding.",
      "image": "",
      "images": [
        "/images/projects/videodigest/cover-01.png",
        "/images/projects/videodigest/cover-02.png"
      ],
      "tag": [],
      "team": [
        {
          "name": "Deepesh Patil",
          "role": "Software Engineer",
          "avatar": "/images/avatar.jpg",
          "linkedin": "https://www.linkedin.com/in/deepesh-patil-103a87258"
        }
      ],
      "link": "https://github.com/deepesh611/Video-Summary-Generator"
    },
    "content": "\r\n## Overview\r\n\r\nVideo Summary Generator is an AI-powered application designed to automatically extract meaningful summaries from video content by combining **visual and audio understanding**.  \r\nInstead of relying on a single modality, the system processes video data through **two parallel pipelines**â€”one focused on frame-level visual analysis and the other on audio transcription and semantic interpretationâ€”to produce more accurate and context-aware summaries.\r\n\r\nThe project explores multimodal AI workflows and efficient pipeline orchestration for real-world media processing tasks.\r\n\r\n## Key Features\r\n\r\n- **Parallel Processing Pipelines** for visual frame extraction and audio analysis\r\n- **Multimodal Understanding** combining visual context and spoken content\r\n- **Automated Video-to-Text Summarization** using LLM-based reasoning\r\n- **Modular Pipeline Design** enabling independent optimization of each stage\r\n- **Scalable Architecture** suitable for batch or on-demand processing\r\n\r\n## Technologies Used\r\n\r\n- **Python** â€“ Core application logic\r\n- **FFmpeg** â€“ Video processing and frame extraction\r\n- **Speech-to-Text Models** â€“ Audio transcription\r\n- **Large Language Models (LLMs)** â€“ Contextual summarization\r\n- **Multimodal AI Pipelines** â€“ Visual and audio data fusion\r\n- **REST APIs** â€“ Pipeline coordination and data exchange\r\n- **Docker** â€“ Containerized execution environment\r\n- **Git & GitHub** â€“ Version control and project management\r\n\r\n## Architecture\r\n\r\nThe system is built around **two parallel pipelines**:\r\n\r\n- The **Visual Pipeline** extracts key frames from the video and analyzes visual cues to identify important scenes or transitions.\r\n- The **Audio Pipeline** transcribes spoken content and performs semantic analysis on the resulting text.\r\n- Outputs from both pipelines are merged and passed to a summarization layer that generates a coherent final summary.\r\n\r\nThis architecture improves robustness by reducing reliance on a single data source and allows each pipeline to scale independently.\r\n\r\n## Current Status\r\n\r\nThe project is currently in a **working prototype stage**, supporting:\r\n\r\n- End-to-end video ingestion and processing\r\n- Parallel execution of visual and audio pipelines\r\n- Generation of concise, human-readable summaries\r\n\r\nFurther improvements are planned to enhance accuracy and performance.\r\n\r\n## Challenges and Learnings\r\n\r\n- Coordinating **parallel processing pipelines** efficiently\r\n- Handling synchronization between visual and audio outputs\r\n- Managing compute and latency trade-offs in media processing\r\n- Structuring multimodal inputs for effective LLM summarization\r\n- Gaining practical experience with **real-world AI pipeline orchestration**\r\n\r\nThis project strengthened my understanding of applied multimodal AI systems.\r\n\r\n## Outcome\r\n\r\nVideo Summary Generator demonstrates a practical application of **multimodal AI and pipeline parallelism** in media analysis.  \r\nFuture plans include smarter frame selection, improved summarization quality, and real-time processing support.\r\n\r\nOverall, the project showcases my ability to design and implement **end-to-end AI systems** that combine multiple data sources to solve complex problems.\r\n"
  }
]